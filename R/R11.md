# R

## 분류분석

- 다수의 변수를 갖는 데이터 셋을 대상으로 특정 변수의 값을 조건으로 지정하여 데이터를 분류하고 트리 형태의 모델을 생성하는 분석 방법
- 의사결정트리(Decision Tree)
- 랜덤 포레스트(Random Forest)
- 인공신경망(Artificial Neural Network)
- 고객을 분류하는 변수, 규칙, 특성을 찾아내고 이를 토대로 미래 잠재 고객의 행동이나 반응을 예측하거나 유도하는데 활용된다.
  - 예) 대출 은행에서 기존 고객들의 데이터를 활용하여 신용상태의 분류모델을 생성한 후 새로운 고객에 대하여 향후 신용상태를 예측하는 데 이용한다 ( 분류 모델 생성 규칙 : 기존 체납횟수, 대출금과 현재 고객의 수입 비율, 대출 사유 등)
  - 예) 과거의 환자들에 대한 종양 검사의 결과를 바탕으로 종양의 악성 또는 양성 여부를 분류하는 모델을 생성하여 새로운 환자에 대한 암을 진단하는데 이용 ( 분류 조건 : 종양의 크기, 모양, 색깔)



### 분류 분석(Classification Analysis) 특징

- Y 변수 존재 : 설명변수(x 변수)와 반응변수(y 변수)가 존재한다.
- 의사결정트리 : 분류 예측모델에 의해서 의사결정트리 형태로 데이터가 분류된다.
- 비모수 검정 : 선형성, 정규성, 등분산성 가정이 필요 없다
- 추론 기능 : 유의수준 판단 기준이 없다 (추론 기능 없음)
- 활용분야 : 이탈고객과 지속고객 분류, 신용상태의 좋고, 나쁨, 번호이동고객과 지속 고객 분류 등



### 분류 분석(Classification Analysis) 절차

- 학습 데이터 생성
- 분류 알고리즘을 통해 예측 모델 생성
- 검정 데이터를 통해 분류규칙의 모델 평가(모형 평가)
- 새로운 데이터에 적용하여 결과 예측

![1569198303244](C:\Users\student\AppData\Roaming\Typora\typora-user-images\1569198303244.png)



### 의사 결정 트리(Decision Tree)

- 나무(Tree) 구조 형태로 분류결과를 도출

- 입력변수 중 가장 영향력 있는 변수를 기준으로 이진분류하여 분류 결과를 나무 구조 형태로 시각화

- 비교적 모델 생성이 쉽고, 단순, 명료하여 현업에서 많이 사용되는 지도학습 모델

- 의사결정규칙을 도표화 하여 분류와 예측을 수행하는 분석방법

- party 패키지 ctree()

- rpart 패키지 rpart()



### party 패키지 ctree()  분류 결과 해석

- 첫번째 번호는 반응변수(종속변수)에 대해서 설명변수(독립변수)가 영향을 미치는 중요 변수의 척도를 나타내는 수치로서 수치가 작을 수록 영향을 미치는 정도가 높고, 순서는 분기되는 순서를 의미한다.

- 두번째는 의사결정 트리의 노드명 (노드 번호 뒤에 * 기호가 오면 해당 노드가 마지막 노드를 의미)
   노드명 뒤에 해당 변수의 임계값이 조건식으로 온다

- 세번째는 노드의 분기 기준(criterion)이 되는 수치

- 네번째는 반응변수(종속변수)의 통계량(statistic)이 표시된다. 



![1569198393246](C:\Users\student\AppData\Roaming\Typora\typora-user-images\1569198393246.png)

### rpart 패키지 rpart()를 이용한 분류 분석

- 재귀분할(recursive partitioning)

- 2수준 요인으로 분산분석을 실행한 결과를 트리 형태로 제공하여 모형을 단순화

- 전체적인 분류기준을 쉽게 분석할 수 있는 장점이 있다



#### 뉴욕의 대기 질을 측정한 데이터셋

```R
> library(party)
필요한 패키지를 로딩중입니다: grid
필요한 패키지를 로딩중입니다: mvtnorm
필요한 패키지를 로딩중입니다: modeltools
필요한 패키지를 로딩중입니다: stats4
필요한 패키지를 로딩중입니다: strucchange
필요한 패키지를 로딩중입니다: zoo

다음의 패키지를 부착합니다: ‘zoo’

The following objects are masked from ‘package:base’:

    as.Date, as.Date.numeric

필요한 패키지를 로딩중입니다: sandwich
> library(datasets)
> #뉴욕의 대기 질을 측정한 데이터셋
> str(airquality) #관측치 15개, 변수 6개
'data.frame':	153 obs. of  6 variables:
 $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...
 $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...
 $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...
 $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...
 $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...
 $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...
> #Ozone, Solar.R(태양광), Wind, Temp, Month, Day
> #온도에 영향을 미치는 변수를 알아보기
> formula <- Temp ~ Solar.R + Wind+Ozone
> 
> #분류모델 생성
> air_tree <- ctree(formula, data=airquality)
> air_tree

	 Conditional inference tree with 5 terminal nodes

Response:  Temp 
Inputs:  Solar.R, Wind, Ozone 
Number of observations:  153 

1) Ozone <= 37; criterion = 1, statistic = 56.086
  2) Wind <= 15.5; criterion = 0.993, statistic = 9.387
    3) Ozone <= 19; criterion = 0.964, statistic = 6.299
      4)*  weights = 29 
    3) Ozone > 19
      5)*  weights = 69 
  2) Wind > 15.5
    6)*  weights = 7 
1) Ozone > 37
  7) Ozone <= 65; criterion = 0.971, statistic = 6.691
    8)*  weights = 22 
  7) Ozone > 65
    9)*  weights = 26 
> 
> plot(air_tree)

#온도에 가장 큰 영향을 미치는 첫번째 영향변수는 Ozone
# 두번째 영향변수는 Wind
# 오존량 37이하이면서 바람의 양이 15.5이상이면 평균온도는 63정도에 해당
#바람의 양이 15.5이하인 경우 평균 온도는 70이상으로 나타남
#태양광은 온도에 영향을 미치지 않는 것으로 분석됨

```



![1569199314284](C:\Users\student\AppData\Roaming\Typora\typora-user-images\1569199314284.png)



#### iris 데이터 셋으로 분류 분석 수행

```R
> set.seed(1234)
> idx <- sample(1:nrow(iris), nrow(iris)*0.7)
> train <- iris[idx, ] #학습 데이터
> test <- iris[-idx, ] #검정 데이터
> 
> #종속변수는 Species, 독립변수는 .....
> formula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
> 
> #분류모델 생성
> iris_ctree <- ctree(formula, data = train)
> iris_ctree

	 Conditional inference tree with 4 terminal nodes

Response:  Species 
Inputs:  Sepal.Length, Sepal.Width, Petal.Length, Petal.Width 
Number of observations:  105 

1) Petal.Length <= 1.9; criterion = 1, statistic = 98.365
  2)*  weights = 34 
1) Petal.Length > 1.9
  3) Petal.Width <= 1.6; criterion = 1, statistic = 47.003
    4) Petal.Length <= 4.6; criterion = 1, statistic = 14.982
      5)*  weights = 28 
    4) Petal.Length > 4.6
      6)*  weights = 8 
  3) Petal.Width > 1.6
    7)*  weights = 35 

# 꽃종 분류에 가장 중요한 독립변수는 Petal.Length, Petal.Width
> plot(iris_ctree, type = "simple")
> plot(iris_ctree)


> # 분류모델 평가 - 예측치 생성, 혼돈 매트릭스 생성
> pred <- predict(iris_ctree, test)
> table(pred, test$Species)
            
pred         setosa versicolor virginica
  setosa         16          0         0
  versicolor      0         15         1
  virginica       0          1        12



> (16 + 15 + 12)/nrow(test)
[1] 0.9555556
#분류정확도 95.56%
```

![1569201952875](assets/1569201952875.png)





#### rpart

- 재귀분할(recursive partitioning)

- 2수준 요인으로 분산분석을 실행한 결과를 트리 형태로 제공하여 모형을 단순화

- 전체적인 분류기준을 쉽게 분석할 수 있는 장점이 있다

```R
> iris.df <- rpart(Species ~., data = iris)
> iris.df
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)  
  2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
  3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)  
    6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
    7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
> 
> plot(iris.df)
> text(iris.df, use.n = T, cex=0.6)
> post(iris.df, file="")
> 
> #줄기에 분기 조건
> #끝 노드에드 반응변수의 결과값이 나타남
> # 꽃 종류 변수를 분류하는 가장 중요한 변수는 Petal.Length와 Petal.Width로 나타난다.

```



![1569206968065](assets/1569206968065.png)





#### 분류분석 연습문제 3

```R
############분류분석 연습문제 3 ########################
weather <- read.csv("./data/weather.csv", header=TRUE)

#RainTomorrow 컬럼을 종속변수로 
# 날씨 요인과 관련없는 Date와 RainToday컬럼을 제외한 나머지 변수를 x변수로 지정하여 분류 모델 생성하고 모델을 평가하시오


> str(weather)
'data.frame':	366 obs. of  15 variables:
 $ Date         : Factor w/ 366 levels "2014-11-01","2014-11-02",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ MinTemp      : num  8 14 13.7 13.3 7.6 6.2 6.1 8.3 8.8 8.4 ...
 $ MaxTemp      : num  24.3 26.9 23.4 15.5 16.1 16.9 18.2 17 19.5 22.8 ...
 $ Rainfall     : num  0 3.6 3.6 39.8 2.8 0 0.2 0 0 16.2 ...
 $ Sunshine     : num  6.3 9.7 3.3 9.1 10.6 8.2 8.4 4.6 4.1 7.7 ...
 $ WindGustDir  : Factor w/ 16 levels "E","ENE","ESE",..: 8 2 8 8 11 10 10 1 9 1 ...
 $ WindGustSpeed: int  30 39 85 54 50 44 43 41 48 31 ...
 $ WindDir      : Factor w/ 16 levels "E","ENE","ESE",..: 8 14 6 14 3 1 3 1 2 3 ...
 $ WindSpeed    : int  20 17 6 24 28 24 26 24 17 6 ...
 $ Humidity     : int  29 36 69 56 49 57 47 57 48 32 ...
 $ Pressure     : num  1015 1008 1007 1007 1018 ...
 $ Cloud        : int  7 3 7 7 7 5 6 7 7 1 ...
 $ Temp         : num  23.6 25.7 20.2 14.1 15.4 14.8 17.3 15.5 18.9 21.7 ...
 $ RainToday    : Factor w/ 2 levels "No","Yes": 1 2 2 2 2 1 1 1 1 2 ...
 $ RainTomorrow : Factor w/ 2 levels "No","Yes": 2 2 2 2 1 1 1 1 2 1 ...
> names(weather)
 [1] "Date"          "MinTemp"       "MaxTemp"      
 [4] "Rainfall"      "Sunshine"      "WindGustDir"  
 [7] "WindGustSpeed" "WindDir"       "WindSpeed"    
[10] "Humidity"      "Pressure"      "Cloud"        
[13] "Temp"          "RainToday"     "RainTomorrow" 
> weather.df <- rpart(RainTomorrow ~ ., data=weather[, c(-1, -14)], cp=0.01) 
> X11()
> plot(weather.df)
> text(weather.df, use.n=T, cex=0.7)
> 
> #분석 결과 : 분기조건이 True이면 왼쪽으로 분류되고, False
> 이면 오른쪽으로 분류된다.
Error: unexpected symbol in "이면 오른쪽으로"
> #rpart()함수의 cp속성값을 높이면 가지 수가 적어지고, 낮추면 가지 수가 많아진다. cp 기본값은 0.01
> 
> weather_pred <- predict(weather.df , weather)
> weather_pred
 
> #y의 범주로 코딩 변환 : Yes(0.5이상), No(0.5미만)
> #rpart의 분류모델 예측치는 비 유무를 0~1사이의 확률값으로 예측하다 
> # 혼돈매트릭스를 이용하여 분류정확도를 구하기 위해 범주화 코딩 변경
> weather_pred2 <- ifelse(weather_pred[,2] >= 0.5, 'Yes', 'No')
> table(weather_pred2, weather$RainTomorrow)
             
weather_pred2  No Yes
          No  278  13
          Yes  22  53

#정확도
> (53 + 278)/nrow(weather)
[1] 0.9043716


```



![data](assets/data.png)





### k겹 교차 검증(k-fold cross validation)

- 테스트를 더 정확하게 설정할수록 세상으로 나왔을 때 더 잘 작동한다고 할 수 있습니다. 

- 딥러닝 혹은 머신러닝 작업을 할 때 늘 어려운 문제 중 하나는 알고리즘을 충분히 테스트하였어도 데이터가 충분치 않으면 좋은 결과를 내기가 어렵습니다

- 데이터의 약 70%를 학습셋으로 써야 했으므로 테스트셋은 겨우 전체 데이터의 30%에 그쳤습니다. 이 정도 테스트만으로는 실제로 얼마나 잘 작동하는지 확신하기는 쉽지 않습니다.

- k겹 교차 검증(k-fold cross validation)  - 테스트 데이터 충분하지 않을 경우  단점을 보완하고자 만든 방법이 바로 입니다. 

- k겹 교차 검증이란 데이터셋을 여러 개로 나누어 하나씩 테스트셋으로 사용하고 나머지를 모두 합해서 학습셋으로 사용하는 방법입니다. 이렇게 하면 가지고 있는 데이터의 100%를 테스트셋으로 사용할 수 있습니다. 

![1569201815571](assets/1569201815571.png)

```R
> #k겹 교차 검증
> library(cvTools)
필요한 패키지를 로딩중입니다: lattice
필요한 패키지를 로딩중입니다: robustbase
> cross <- cvFolds(nrow(iris), K=3, R=2)
> str(cross)
List of 5
 $ n      : num 150
 $ K      : num 3
 $ R      : num 2
 $ subsets: int [1:150, 1:2] 21 102 134 9 19 22 40 29 109 38 ...
 $ which  : int [1:150] 1 2 3 1 2 3 1 2 3 1 ...
 - attr(*, "class")= chr "cvFolds"




> #k겹 교차 검증
> library(cvTools)
> cross <- cvFolds(nrow(iris), K=3, R=2)
> str(cross)
List of 5
 $ n      : num 150
 $ K      : num 3
 $ R      : num 2
 $ subsets: int [1:150, 1:2] 67 25 82 3 27 124 86 119 65 6 ...
 $ which  : int [1:150] 1 2 3 1 2 3 1 2 3 1 ...
 - attr(*, "class")= chr "cvFolds"
> cross  #교차검정 데이터 확인

Repeated 3-fold CV with 2 replications:    
Fold      1   2
   1     67   8
   2     25 143
   3     82 128
   1      3  68
   2     27 109
   3    124  56
   1     86  22
   2    119  53
   3     65  40
   1      6  27
   2    112 141
   3     64  84
   1    123 148
   2    114  95
   3     98 123
   1     24  74
   2     57 137
   3     31  32
   1     71 105
   2     97  45
   3    118 126
   1     41  39
   2     52 129
   3     35  26
   1     55  80
   2     92 127
   3    101  36
   1    135  28
   2     54   2
   3     94  93
   1    128 133
   2     69  97
   3     23  96
   1     37 118
   2    130  29
   3    148  81
   1    131  37
   2     88  69
   3     53 149
   1     90  66
   2     17  11
   3    133  64
   1    122 122
   2    143 150
   3    144 116
   1    129  10
   2     40  78
   3     93  47
   1    127  48
   2    139 135
   3     89  71
   1    111  41
   2     59  24
   3     16  30
   1     68  94
   2     28  51
   3      9  92
   1     91 119
   2    120 136
   3     33 102
   1     48 121
   2    134  50
   3     56  57
   1    145  13
   2     81  54
   3     87 120
   1     72 111
   2    146  23
   3    136  99
   1     58 104
   2     45 107
   3     84  86
   1     13 134
   2    147 106
   3     62 115
   1     30 108
   2    132 100
   3     34 147
   1     76  91
   2    105 131
   3    142   6
   1    138 103
   2     61 139
   3    137   7
   1     83 145
   2     49  43
   3     78 130
   1     19  44
   2     18  17
   3     63  77
   1    103  73
   2     85 142
   3    121 114
   1    108  90
   2    109  31
   3    116   1
   1    115  14
   2     32  59
   3     77  12
   1     12  89
   2    110  52
   3     44 117
   1      5  58
   2     96  76
   3     38  75
   1     39  42
   2     10  46
   3     74 138
   1      8  55
   2     22  21
   3     42 146
   1     95  34
   2     50  20
   3     11 112
   1      7 101
   2     43  35
   3     47  79
   1     79 125
   2    150  18
   3    141  85
   1    107   4
   2      2  19
   3    102  16
   1    126  67
   2    117 113
   3     73  82
   1     21  70
   2     26 110
   3     36  60
   1    104  63
   2     29 140
   3    125  65
   1     14  62
   2    113  33
   3     60  98
   1     80 124
   2     46 132
   3    140 144
   1     66  49
   2     99  88
   3     51  38
   1     75  72
   2      4   9
   3    100   5
   1     15  15
   2     20  61
   3      1  25
   1     70  87
   2    106   3
   3    149  83
> length(cross$which)
[1] 150
> dim(cross$subsets)
[1] 150   2
> table(cross$which)

 1  2  3 
50 50 50 
> R=1:2
> K=1:3
> CNT=0               #카운트 변수
> ACC <- numeric()    #정확도 저장

> for(r in R) {
+   cat('\n R=', r, '\r')
+   for (k in K) {
+     datas_idx <- cross$subsets[cross$which==k, r]
+     test <- iris[datas_idx, ]   #테스트 데이터 생성
+     cat('test:', nrow(test), '\n')
+     
+     formula <- Species ~ .
+     train <- iris[-datas_idx, ]  #훈련 데이터 생성
+     cat('train:', nrow(train), '\n')
+     
+     model <- ctree(formula, data=train)
+     pred <- predict(model, test)
+     t <- table(pred, test$Species)
+     print(t)              #혼돈 메트릭스
+     CNT <- CNT+1
+     ACC[CNT] <- (t[1,1]+t[2,2]+t[3,3]) /sum(t)
+     
+   }
+ }

test: 50 
train: 100 
            
pred         setosa versicolor virginica
  setosa         17          0         0
  versicolor      0         16         2
  virginica       0          1        14
test: 50 
train: 100 
            
pred         setosa versicolor virginica
  setosa         17          0         0
  versicolor      2         13         3
  virginica       0          0        15
test: 50 
train: 100 
            
pred         setosa versicolor virginica
  setosa         14          0         0
  versicolor      0         19         0
  virginica       0          1        16

test: 50 
train: 100 
            
pred         setosa versicolor virginica
  setosa         17          0         0
  versicolor      0         17         1
  virginica       0          0        15
test: 50 
train: 100 
            
pred         setosa versicolor virginica
  setosa         19          0         0
  versicolor      0         12         1
  virginica       0          1        17
test: 50 
train: 100 
            
pred         setosa versicolor virginica
  setosa         14          0         0
  versicolor      0         19         2
  virginica       0          1        14
> CNT   #테스트 데이터 3셋 생성 모델, 예측 비교를 2번 반복, 즉 6회 수행 
[1] 9
> ACC  #6회 수행 정확도 확인
[1] 0.9555556        NA 0.9555556 0.9400000 0.9000000
[6] 0.9800000 0.9800000 0.9600000 0.9400000
> #6회 정확도의 평균
> mean(ACC, na.rm=T)
[1] 0.9513889

```





````R
# ggplot2::mpg 데이터 셋
# model(모델), displ(엔진 크기) , cyl(실린더 수), drv(구동 방식)
# 종속변수 : 고속도로 주행거리(hwy)


> library(ggplot2)
> data(mpg)
> t <-sample(1:nrow(mpg), 120)
> train <- mpg[t, ]
> test <- mpg[-t, ]
> test$drv <- factor(test$drv)  #구동방식 범주형 변환
> formula <- hwy ~ displ+cyl+drv
> hwy_ctree <- ctree(formula, data=test)
> plot(hwy_ctree)

#분석 결과 : 엔진 크기가 작으면서 전륜구동(f)이나 후륜(r) 방식이면서 실린더의 수가 적은 경우 고속도로 주행거리가 좋아지고,
#엔진 크기가 크고, 사륜구동 방식이면 실린더 수가 많은 경우 고속도로 주행거리가 적은 것으로 분석된다.
````



![1569204190440](assets/1569204190440.png)



```R
> library(arules)
필요한 패키지를 로딩중입니다: Matrix

다음의 패키지를 부착합니다: ‘arules’

The following object is masked from ‘package:modeltools’:

    info

The following objects are masked from ‘package:base’:

    abbreviate, write

> data("AdultUCI")

#성인 대상 인구 소득에 관한 설문 조사 데이터
#48,842 관측치와 15개변수
#age, workclass(직업 :4개), education(교육수준: 16개), marital-status(결혼상태: 6개), occupation(직업:12개), relationship(관계: 6개), race(인종:아시아계, 백인), sex(성별), capital-gain(자본이득), capital-loss(자본손실), fnlwgt(미지의 변수), hours-per-week(주당 근무시간), native-country(국가), income(소득)

#10,000개 관측치를 샘플링해서
#자본이득에 영향을 미치는 변수를 분석하기 위해 
#capital-gain, hours-per-week, education-num, race, age, income 변수로만 구성된 데이터프레임을 생성한후 분류모델 생성하고 예측하시오

> set.seed(1234)
> choice <- sample(1:nrow(AdultUCI), 10000)

> adult.df <- AdultUCI[choice, ]
> str(adult.df)
'data.frame':	10000 obs. of  15 variables:
 $ age           : int  76 34 44 44 50 36 17 26 43 25 ...
 $ workclass     : Factor w/ 8 levels "Federal-gov",..: 6 6 4 4 4 4 4 4 4 4 ...
 $ fnlwgt        : int  106430 201292 318046 368757 115284 207853 158704 147821 160246 135645 ...
 $ education     : Ord.factor w/ 16 levels "Preschool"<"1st-4th"<..: 5 12 13 13 15 14 6 14 13 15 ...
 $ education-num : int  5 11 10 10 14 13 6 13 10 14 ...
 $ marital-status: Factor w/ 7 levels "Divorced","Married-AF-spouse",..: 3 3 3 3 3 3 5 5 1 5 ...
 $ occupation    : Factor w/ 14 levels "Adm-clerical",..: 5 5 12 7 3 12 12 12 10 12 ...
 $ relationship  : Factor w/ 6 levels "Husband","Not-in-family",..: 1 1 1 1 1 1 4 4 5 2 ...
 $ race          : Factor w/ 5 levels "Amer-Indian-Eskimo",..: 5 5 5 5 5 5 5 5 3 5 ...
 $ sex           : Factor w/ 2 levels "Female","Male": 2 2 2 2 2 2 2 1 1 2 ...
 $ capital-gain  : int  0 0 0 0 0 15024 0 0 0 0 ...
 $ capital-loss  : int  0 0 0 0 0 0 0 0 0 0 ...
 $ hours-per-week: int  40 50 35 40 40 65 20 45 40 20 ...
 $ native-country: Factor w/ 41 levels "Cambodia","Canada",..: 39 39 39 39 39 39 39 NA 39 39 ...
 $ income        : Ord.factor w/ 2 levels "small"<"large": NA NA NA 1 NA NA 1 1 1 1 ...
> capital <- adult.df$'capital-gain'
> hours <- adult.df$'hours-per-week'
> education <- adult.df$'education-num'
> race <- adult.df$race
> age <- adult.df$age
> income <- adult.df$income
> adult_df <- data.frame(capital=capital, age=age , hours=hours,
+                        education=education, income=income)
> str(adult_df)
'data.frame':	10000 obs. of  5 variables:
 $ capital  : int  0 0 0 0 0 15024 0 0 0 0 ...
 $ age      : int  76 34 44 44 50 36 17 26 43 25 ...
 $ hours    : int  40 50 35 40 40 65 20 45 40 20 ...
 $ education: int  5 11 10 10 14 13 6 13 10 14 ...
 $ income   : Ord.factor w/ 2 levels "small"<"large": NA NA NA 1 NA NA 1 1 1 1 ...
> formula <- capital ~ income+education+hours+age
> adult_ctree <- ctree(formula, data=adult_df)
> plot(adult_ctree)
> #분류 모델의 조건에 맞는 subset 생성
> adultResult <- subset(adult_df, adult_df$income=='large' &  adult_df$education > 14)
> length(adultResult$education)
[1] 139
> summary(adultResult$capital) 
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      0       0       0    9241    6042   99999 
> boxplot(adultResult$capital)


```



![1569206500286](assets/1569206500286.png)



![1569206521772](assets/1569206521772.png)







### 랜덤 포레스트(Random Forest)

- 의사결정트리에서 파생된 앙상블 학습기법을 적용한 모델

- 앙상블 학습 기법 – 새로운 데이터에 대해서 여러 개의 트리(Forest)로 학습을 수행한 후 학습 결과들을 종합해서 예측하는 모델

- 기존의 의사결정트리 방식에 비해서 많은 데이터를 이용하여 학습을 수행하기 때문에 비교적 예측력이 뛰어나고, 과적합(overfitting)문제를 해결할 수 있다

- 과적합 문제 – 작은 데이터 셋은 높은 정확도가 나타나지만 큰 데이터셋에서는 정확도가 떨어지는 현상을 의미

```R
randomForest(formula, data, ntree, mtry, na.action, importance)
```

- formula : y~x 형식으로 반응변수와 설명변수 식

- data : 모델 생성에 사용될 데이터 셋

- ntree :  복원추출하여 생성할 트리 수 지정

- mtry : 자식 노드를 분류할 변수 수 지정

- na.action :  결측치를 제거할 함수 지정

- importance : 분류모델 생성과정에서 중요 변수 정보 제공 여부





```R
> library(randomForest)
> data(iris)
> model <- randomForest(Species ~., data = iris)
> model

Call:
 randomForest(formula = Species ~ ., data = iris) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
> #Number of trees는 학습데이터로(Forest)로 복원 방식으로 500개 생성했다는 의미
> #No, of variables tried at each split는 두 개의 변수를 이용하여 트리의 자식노드가 분류되었다는 의미 (ntree:500, mtry:2)
> #error.rate는 모델의 분류정확도 오차 비율을 의미
> #Confusion matrix(혼돈 메트릭스)
> #분류 정확도는 (setosa+versicolor+virgina)/150 : 96%
> (50+47+46)/150
[1] 0.9533333


> model2<-randomForest(Species~., data=iris, ntree=300, mtry=4, na.action=na.omit)
> model2

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = 300,      mtry = 4, na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 300
No. of variables tried at each split: 4

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08



> #중요변수 생성으로 랜덤 포레스트 모델 생성
> 
> model3 <- randomForest(Species~., data = iris, importance = T, na.action = na.omit)
> model3

Call:
 randomForest(formula = Species ~ ., data = iris, importance = T,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08

> #중요 변수 보기 - importance속성은 분류모델 생성하는 과정에서 입력변수 중 가장 중요한 변수가 어떤 변수인가를 알려주는 역할을 한다.
> 
> importance(model3)
                setosa versicolor virginica
Sepal.Length  5.959432   8.358710  8.780367
Sepal.Width   4.536569   1.684909  4.403012
Petal.Length 21.480683  34.195190 27.393482
Petal.Width  22.978192  31.944509 30.407446
             MeanDecreaseAccuracy MeanDecreaseGini
Sepal.Length            10.758066         9.307593
Sepal.Width              5.100748         2.307766
Petal.Length            32.484872        43.096519
Petal.Width             33.233729        44.558629

# MeanDecreaseAccuracy - 분류정확도를 개선하는데 기여한 변수를 수치로 제공



> for(i in param$n) {
+   cat('ntree=', i, '\n')
+   for(j in param$m) {
+       cat('mtry = ', j, '\n')
+       model_iris <- randomForest(Species~., data=iris, ntree=i, mtry=j, na.action=na.omit)
+       print(model_iris)
+   }
+ }
ntree= 400 
mtry =  2 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 400
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06
mtry =  3 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 400
No. of variables tried at each split: 3

        OOB estimate of  error rate: 4%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06
mtry =  4 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 400
No. of variables tried at each split: 4

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
ntree= 500 
mtry =  2 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06
mtry =  3 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 3

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
mtry =  4 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 4

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
ntree= 600 
mtry =  2 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 600
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
mtry =  3 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 600
No. of variables tried at each split: 3

        OOB estimate of  error rate: 4%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06
mtry =  4 

Call:
 randomForest(formula = Species ~ ., data = iris, ntree = i, mtry = j,      na.action = na.omit) 
               Type of random forest: classification
                     Number of trees: 600
No. of variables tried at each split: 4

        OOB estimate of  error rate: 4.67%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          4        46        0.08
> 


```





#### 랜덤 포레스트(Random Forest) 학습데이터 구성방법

- 표본에서 일부분만 복원추출 방법으로 랜덤하게 샘플링하는 방식인 부트스트랩 표본(bootstrap sample) 방식으로 학습데이터로 사용될 트리(Forest)를 생성한다.

- 입력변수 중에서 일부 변수만 적용하여 트리의 자식노드(child node)를 분류한다.







