# R

## 분류분석

- 다수의 변수를 갖는 데이터 셋을 대상으로 특정 변수의 값을 조건으로 지정하여 데이터를 분류하고 트리 형태의 모델을 생성하는 분석 방법
- 의사결정트리(Decision Tree)
- 랜덤 포레스트(Random Forest)
- 인공신경망(Artificial Neural Network)
- 고객을 분류하는 변수, 규칙, 특성을 찾아내고 이를 토대로 미래 잠재 고객의 행동이나 반응을 예측하거나 유도하는데 활용된다.
  - 예) 대출 은행에서 기존 고객들의 데이터를 활용하여 신용상태의 분류모델을 생성한 후 새로운 고객에 대하여 향후 신용상태를 예측하는 데 이용한다 ( 분류 모델 생성 규칙 : 기존 체납횟수, 대출금과 현재 고객의 수입 비율, 대출 사유 등)
  - 예) 과거의 환자들에 대한 종양 검사의 결과를 바탕으로 종양의 악성 또는 양성 여부를 분류하는 모델을 생성하여 새로운 환자에 대한 암을 진단하는데 이용 ( 분류 조건 : 종양의 크기, 모양, 색깔)



### 분류 분석(Classification Analysis) 특징

- Y 변수 존재 : 설명변수(x 변수)와 반응변수(y 변수)가 존재한다.
- 의사결정트리 : 분류 예측모델에 의해서 의사결정트리 형태로 데이터가 분류된다.
- 비모수 검정 : 선형성, 정규성, 등분산성 가정이 필요 없다
- 추론 기능 : 유의수준 판단 기준이 없다 (추론 기능 없음)
- 활용분야 : 이탈고객과 지속고객 분류, 신용상태의 좋고, 나쁨, 번호이동고객과 지속 고객 분류 등



### 분류 분석(Classification Analysis) 절차

- 학습 데이터 생성
- 분류 알고리즘을 통해 예측 모델 생성
- 검정 데이터를 통해 분류규칙의 모델 평가(모형 평가)
- 새로운 데이터에 적용하여 결과 예측

![1569198303244](C:\Users\student\AppData\Roaming\Typora\typora-user-images\1569198303244.png)



### 의사 결정 트리(Decision Tree)

- 나무(Tree) 구조 형태로 분류결과를 도출

- 입력변수 중 가장 영향력 있는 변수를 기준으로 이진분류하여 분류 결과를 나무 구조 형태로 시각화

- 비교적 모델 생성이 쉽고, 단순, 명료하여 현업에서 많이 사용되는 지도학습 모델

- 의사결정규칙을 도표화 하여 분류와 예측을 수행하는 분석방법

- party 패키지 ctree()

- rpart 패키지 rpart()



### party 패키지 ctree()  분류 결과 해석

- 첫번째 번호는 반응변수(종속변수)에 대해서 설명변수(독립변수)가 영향을 미치는 중요 변수의 척도를 나타내는 수치로서 수치가 작을 수록 영향을 미치는 정도가 높고, 순서는 분기되는 순서를 의미한다.

- 두번째는 의사결정 트리의 노드명 (노드 번호 뒤에 * 기호가 오면 해당 노드가 마지막 노드를 의미)
   노드명 뒤에 해당 변수의 임계값이 조건식으로 온다

- 세번째는 노드의 분기 기준(criterion)이 되는 수치

- 네번째는 반응변수(종속변수)의 통계량(statistic)이 표시된다. 



![1569198393246](C:\Users\student\AppData\Roaming\Typora\typora-user-images\1569198393246.png)

### rpart 패키지 rpart()를 이용한 분류 분석

- 재귀분할(recursive partitioning)

- 2수준 요인으로 분산분석을 실행한 결과를 트리 형태로 제공하여 모형을 단순화

- 전체적인 분류기준을 쉽게 분석할 수 있는 장점이 있다



#### 뉴욕의 대기 질을 측정한 데이터셋

```R
> library(party)
필요한 패키지를 로딩중입니다: grid
필요한 패키지를 로딩중입니다: mvtnorm
필요한 패키지를 로딩중입니다: modeltools
필요한 패키지를 로딩중입니다: stats4
필요한 패키지를 로딩중입니다: strucchange
필요한 패키지를 로딩중입니다: zoo

다음의 패키지를 부착합니다: ‘zoo’

The following objects are masked from ‘package:base’:

    as.Date, as.Date.numeric

필요한 패키지를 로딩중입니다: sandwich
> library(datasets)
> #뉴욕의 대기 질을 측정한 데이터셋
> str(airquality) #관측치 15개, 변수 6개
'data.frame':	153 obs. of  6 variables:
 $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...
 $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...
 $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...
 $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...
 $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...
 $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...
> #Ozone, Solar.R(태양광), Wind, Temp, Month, Day
> #온도에 영향을 미치는 변수를 알아보기
> formula <- Temp ~ Solar.R + Wind+Ozone
> 
> #분류모델 생성
> air_tree <- ctree(formula, data=airquality)
> air_tree

	 Conditional inference tree with 5 terminal nodes

Response:  Temp 
Inputs:  Solar.R, Wind, Ozone 
Number of observations:  153 

1) Ozone <= 37; criterion = 1, statistic = 56.086
  2) Wind <= 15.5; criterion = 0.993, statistic = 9.387
    3) Ozone <= 19; criterion = 0.964, statistic = 6.299
      4)*  weights = 29 
    3) Ozone > 19
      5)*  weights = 69 
  2) Wind > 15.5
    6)*  weights = 7 
1) Ozone > 37
  7) Ozone <= 65; criterion = 0.971, statistic = 6.691
    8)*  weights = 22 
  7) Ozone > 65
    9)*  weights = 26 
> 
> plot(air_tree)

#온도에 가장 큰 영향을 미치는 첫번째 영향변수는 Ozone
# 두번째 영향변수는 Wind
# 오존량 37이하이면서 바람의 양이 15.5이상이면 평균온도는 63정도에 해당
#바람의 양이 15.5이하인 경우 평균 온도는 70이상으로 나타남
#태양광은 온도에 영향을 미치지 않는 것으로 분석됨

```



![1569199314284](C:\Users\student\AppData\Roaming\Typora\typora-user-images\1569199314284.png)



#### iris 데이터 셋으로 분류 분석 수행

```R
> set.seed(1234)
> idx <- sample(1:nrow(iris), nrow(iris)*0.7)
> train <- iris[idx, ] #학습 데이터
> test <- iris[-idx, ] #검정 데이터
> 
> #종속변수는 Species, 독립변수는 .....
> formula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
> 
> #분류모델 생성
> iris_ctree <- ctree(formula, data = train)
> iris_ctree

	 Conditional inference tree with 4 terminal nodes

Response:  Species 
Inputs:  Sepal.Length, Sepal.Width, Petal.Length, Petal.Width 
Number of observations:  105 

1) Petal.Length <= 1.9; criterion = 1, statistic = 98.365
  2)*  weights = 34 
1) Petal.Length > 1.9
  3) Petal.Width <= 1.6; criterion = 1, statistic = 47.003
    4) Petal.Length <= 4.6; criterion = 1, statistic = 14.982
      5)*  weights = 28 
    4) Petal.Length > 4.6
      6)*  weights = 8 
  3) Petal.Width > 1.6
    7)*  weights = 35 

# 꽃종 분류에 가장 중요한 독립변수는 Petal.Length, Petal.Width
> plot(iris_ctree, type = "simple")
> plot(iris_ctree)


> # 분류모델 평가 - 예측치 생성, 혼돈 매트릭스 생성
> pred <- predict(iris_ctree, test)
> table(pred, test$Species)
            
pred         setosa versicolor virginica
  setosa         16          0         0
  versicolor      0         15         1
  virginica       0          1        12



> (16 + 15 + 12)/nrow(test)
[1] 0.9555556
#분류정확도 95.56%
```

![1569201952875](assets/1569201952875.png)







### k겹 교차 검증(k-fold cross validation)

- 테스트를 더 정확하게 설정할수록 세상으로 나왔을 때 더 잘 작동한다고 할 수 있습니다. 

- 딥러닝 혹은 머신러닝 작업을 할 때 늘 어려운 문제 중 하나는 알고리즘을 충분히 테스트하였어도 데이터가 충분치 않으면 좋은 결과를 내기가 어렵습니다

- 데이터의 약 70%를 학습셋으로 써야 했으므로 테스트셋은 겨우 전체 데이터의 30%에 그쳤습니다. 이 정도 테스트만으로는 실제로 얼마나 잘 작동하는지 확신하기는 쉽지 않습니다.

- k겹 교차 검증(k-fold cross validation)  - 테스트 데이터 충분하지 않을 경우  단점을 보완하고자 만든 방법이 바로 입니다. 

- k겹 교차 검증이란 데이터셋을 여러 개로 나누어 하나씩 테스트셋으로 사용하고 나머지를 모두 합해서 학습셋으로 사용하는 방법입니다. 이렇게 하면 가지고 있는 데이터의 100%를 테스트셋으로 사용할 수 있습니다. 

![1569201815571](assets/1569201815571.png)

```R
> #k겹 교차 검증
> library(cvTools)
필요한 패키지를 로딩중입니다: lattice
필요한 패키지를 로딩중입니다: robustbase
> cross <- cvFolds(nrow(iris), K=3, R=2)
> str(cross)
List of 5
 $ n      : num 150
 $ K      : num 3
 $ R      : num 2
 $ subsets: int [1:150, 1:2] 21 102 134 9 19 22 40 29 109 38 ...
 $ which  : int [1:150] 1 2 3 1 2 3 1 2 3 1 ...
 - attr(*, "class")= chr "cvFolds"


```





### 랜덤 포레스트(Random Forest)

- 의사결정트리에서 파생된 앙상블 학습기법을 적용한 모델

- 앙상블 학습 기법 – 새로운 데이터에 대해서 여러 개의 트리(Forest)로 학습을 수행한 후 학습 결과들을 종합해서 예측하는 모델

- 기존의 의사결정트리 방식에 비해서 많은 데이터를 이용하여 학습을 수행하기 때문에 비교적 예측력이 뛰어나고, 과적합(overfitting)문제를 해결할 수 있다

- 과적합 문제 – 작은 데이터 셋은 높은 정확도가 나타나지만 큰 데이터셋에서는 정확도가 떨어지는 현상을 의미





#### 랜덤 포레스트(Random Forest) 학습데이터 구성방법

- 표본에서 일부분만 복원추출 방법으로 랜덤하게 샘플링하는 방식인 부트스트랩 표본(bootstrap sample) 방식으로 학습데이터로 사용될 트리(Forest)를 생성한다.

- 입력변수 중에서 일부 변수만 적용하여 트리의 자식노드(child node)를 분류한다.





